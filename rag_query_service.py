#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGæŸ¥è¯¢æœåŠ¡æ¨¡å— (OpenAIç‰ˆæœ¬)
ä¸ºåç«¯APIæä¾›æ™ºèƒ½çŸ¥è¯†åº“æŸ¥è¯¢åŠŸèƒ½
"""

# ä¼˜å…ˆåŠ è½½ç¯å¢ƒå˜é‡ - å¿…é¡»åœ¨æ‰€æœ‰å…¶ä»–ä»£ç ä¹‹å‰
from dotenv import load_dotenv
load_dotenv()

import os
import sys
import json
import logging
from pathlib import Path
from typing import List, Dict, Any

# å®Œå…¨ç¦ç”¨æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºåˆ°stdout
import warnings
warnings.filterwarnings('ignore')

# é‡å®šå‘æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºåˆ°null
class DevNull:
    def write(self, msg):
        pass
    def flush(self):
        pass

# åœ¨å¯¼å…¥LlamaIndexä¹‹å‰ï¼Œå…ˆè®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨verboseè¾“å‡º
os.environ["LLAMA_INDEX_LOGGING_LEVEL"] = "CRITICAL"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ä¸´æ—¶é‡å®šå‘stdoutï¼Œé˜²æ­¢LlamaIndexçš„å¯¼å…¥æ—¶è¾“å‡º
original_stdout = sys.stdout
sys.stdout = DevNull()

try:
    # LlamaIndexæ ¸å¿ƒæ¨¡å—
    from llama_index.core import (
        StorageContext,
        load_index_from_storage,
        Settings
    )
    from llama_index.embeddings.openai import OpenAIEmbedding
    from llama_index.core.llms import MockLLM
finally:
    # æ¢å¤stdout
    sys.stdout = original_stdout

# é…ç½®æ—¥å¿—ï¼Œé‡å®šå‘åˆ°stderré¿å…æ±¡æŸ“stdout
logging.basicConfig(
    level=logging.CRITICAL,  # åªè¾“å‡ºä¸¥é‡é”™è¯¯
    stream=sys.stderr,       # é‡å®šå‘åˆ°stderr
    format='%(message)s'
)
logger = logging.getLogger(__name__)

class RAGQueryService:
    """RAGæŸ¥è¯¢æœåŠ¡ç±» - OpenAIç‰ˆæœ¬"""
    
    def __init__(self, storage_path: str = "storage"):
        """
        åˆå§‹åŒ–RAGæŸ¥è¯¢æœåŠ¡
        
        Args:
            storage_path: ç´¢å¼•å­˜å‚¨è·¯å¾„
        """
        self.storage_path = Path(storage_path)
        self.index = None
        self.query_engine = None
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.is_initialized = False
        self.initialization_error = None
        
        try:
            # é…ç½®LlamaIndexè®¾ç½®
            self._setup_llama_index()
            
            # åŠ è½½ç´¢å¼•
            if self.load_index():
                self.is_initialized = True
            else:
                self.initialization_error = "ç´¢å¼•åŠ è½½å¤±è´¥"
                
        except Exception as e:
            self.initialization_error = str(e)
            logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _setup_llama_index(self):
        """é…ç½®LlamaIndexå…¨å±€è®¾ç½® - ä½¿ç”¨OpenAI"""
        
        # æ£€æŸ¥OpenAI APIå¯†é’¥
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®æ‚¨çš„OpenAI APIå¯†é’¥")
        
        # è·å–OpenAI APIåŸºç¡€URL
        openai_api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        
        # ä½¿ç”¨OpenAI embedding
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-small",
            api_key=openai_api_key,
            api_base=openai_api_base
        )
        
        # ä½¿ç”¨MockLLMé¿å…LLMè°ƒç”¨ï¼ŒåŒæ—¶è®¾ç½®æå…¶ä¿å®ˆçš„å‚æ•°
        Settings.llm = MockLLM(max_tokens=128)
        
        # è®¾ç½®æå…¶ä¿å®ˆçš„ä¸Šä¸‹æ–‡å‚æ•°é¿å…è¶…å‡ºé™åˆ¶
        Settings.chunk_size = 128    # æå°çš„chunk size
        Settings.chunk_overlap = 20   # æå°çš„overlap
        Settings.context_window = 512 # æå°çš„ä¸Šä¸‹æ–‡çª—å£
        Settings.num_output = 64      # æå°çš„è¾“å‡ºé•¿åº¦
        
        logger.debug("LlamaIndexé…ç½®å®Œæˆ (OpenAI Embedding)")
    
    def load_index(self):
        """åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•"""
        if not (self.storage_path / "index_store.json").exists():
            return False
        
        try:
            # ä¸´æ—¶é‡å®šå‘stdoutï¼Œé˜²æ­¢loadingä¿¡æ¯è¾“å‡º
            original_stdout = sys.stdout
            sys.stdout = DevNull()
            
            try:
                # ä»å­˜å‚¨ä¸­åŠ è½½ç´¢å¼•
                storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_path))
                self.index = load_index_from_storage(storage_context)
                
                # åˆ›å»ºæŸ¥è¯¢å¼•æ“ - ä½¿ç”¨æœ€æœ€ä¿å®ˆçš„è®¾ç½®é¿å…ä¸Šä¸‹æ–‡é—®é¢˜
                self.query_engine = self.index.as_query_engine(
                    similarity_top_k=5,         # ä¼˜åŒ–ï¼šå–5ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µï¼Œæå‡çŸ¥è¯†åº“è¦†ç›–åº¦
                    response_mode="compact",    # ä½¿ç”¨compactæ¨¡å¼
                    text_qa_template=None,      # ä¸ä½¿ç”¨æ¨¡æ¿
                    streaming=False,
                    verbose=False               # ç¦ç”¨verboseè¾“å‡º
                )
            finally:
                # æ¢å¤stdout
                sys.stdout = original_stdout
            
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
            return False
    
    def query(self, question: str, context: str = "", diagnostic_mode: bool = False) -> Dict[str, Any]:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            context: é¢å¤–ä¸Šä¸‹æ–‡
            diagnostic_mode: æ˜¯å¦å¯ç”¨è¯Šæ–­æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†æ£€ç´¢ä¿¡æ¯åˆ°stderr
            
        Returns:
            æŸ¥è¯¢ç»“æœå­—å…¸
        """
        if not self.is_initialized:
            return {
                "error": f"RAGç³»ç»Ÿæœªåˆå§‹åŒ–: {self.initialization_error}",
                "answer": "",
                "sources": [],
                "query": question,
                "context": context,
                "sources_count": 0
            }
        
        try:
            # æ„å»ºæŸ¥è¯¢ - æˆªæ–­è¿‡é•¿çš„å†…å®¹é¿å…ä¸Šä¸‹æ–‡è¶…é™
            if len(question) > 100:
                question = question[:100] + "..."
            
            if context and len(context) > 50:
                context = context[:50] + "..."
            
            full_query = f"{question}\n{context}" if context else question
            
            # ä¸´æ—¶é‡å®šå‘stdoutï¼Œé˜²æ­¢æŸ¥è¯¢è¿‡ç¨‹çš„è¾“å‡º
            original_stdout = sys.stdout
            sys.stdout = DevNull()
            
            try:
                # æ‰§è¡ŒæŸ¥è¯¢
                response = self.query_engine.query(full_query)
            finally:
                # æ¢å¤stdout
                sys.stdout = original_stdout
            
            # æå–æºä¿¡æ¯
            sources = []
            if hasattr(response, 'source_nodes'):
                # ğŸ” è¯Šæ–­æ¨¡å¼ï¼šè¯¦ç»†è¾“å‡ºæ£€ç´¢ä¿¡æ¯åˆ°stderr
                if diagnostic_mode:
                    print("\n" + "="*80, file=sys.stderr)
                    print("ğŸ”¬ RAGç³»ç»Ÿæ£€ç´¢è¯Šæ–­æŠ¥å‘Š", file=sys.stderr)
                    print("="*80, file=sys.stderr)
                    print(f"ğŸ“ æŸ¥è¯¢é—®é¢˜: {question}", file=sys.stderr)
                    if context.strip():
                        print(f"ğŸ“„ ä¸Šä¸‹æ–‡: {context}", file=sys.stderr)
                    print(f"ğŸ” å®Œæ•´æŸ¥è¯¢: {full_query}", file=sys.stderr)
                    print(f"ğŸ“Š æ£€ç´¢åˆ° {len(response.source_nodes)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ\n", file=sys.stderr)
                
                for i, node in enumerate(response.source_nodes, 1):
                    source_info = {
                        "content": node.text[:100] + "..." if len(node.text) > 100 else node.text,
                        "score": float(node.score) if hasattr(node, 'score') else 0.0
                    }
                    
                    # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
                    if hasattr(node, 'metadata') and 'file_path' in node.metadata:
                        source_info['file_path'] = node.metadata['file_path']
                    
                    # ğŸ” è¯Šæ–­æ¨¡å¼ï¼šè¯¦ç»†è¾“å‡ºæ¯ä¸ªæ£€ç´¢ç‰‡æ®µ
                    if diagnostic_mode:
                        print(f"ğŸ“„ ç‰‡æ®µ {i}", file=sys.stderr)
                        print("-" * 60, file=sys.stderr)
                        
                        # ç›¸å…³æ€§å¾—åˆ†
                        score = float(node.score) if hasattr(node, 'score') else 0.0
                        print(f"ğŸ¯ ç›¸å…³æ€§å¾—åˆ†: {score:.4f}", file=sys.stderr)
                        
                        # æ¥æºå…ƒæ•°æ®
                        metadata = getattr(node, 'metadata', {})
                        if 'file_path' in metadata:
                            file_name = os.path.basename(metadata['file_path'])
                            print(f"ğŸ“ æ¥æºæ–‡ä»¶: {file_name}", file=sys.stderr)
                            print(f"ğŸ“‚ å®Œæ•´è·¯å¾„: {metadata['file_path']}", file=sys.stderr)
                        
                        # å…¶ä»–å…ƒæ•°æ®ä¿¡æ¯
                        if metadata:
                            for key, value in metadata.items():
                                if key != 'file_path':
                                    print(f"ğŸ“‹ {key}: {value}", file=sys.stderr)
                        
                        # åŸå§‹æ–‡æœ¬å—ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼Œé¿å…è¿‡é•¿ï¼‰
                        display_text = node.text[:500] + "..." if len(node.text) > 500 else node.text
                        print(f"ğŸ“ åŸå§‹æ–‡æœ¬å— (é•¿åº¦: {len(node.text)} å­—ç¬¦):", file=sys.stderr)
                        print("-" * 40, file=sys.stderr)
                        print(display_text, file=sys.stderr)
                        print("-" * 40, file=sys.stderr)
                        
                        # æ–‡æœ¬å—IDï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(node, 'node_id'):
                            print(f"ğŸ†” èŠ‚ç‚¹ID: {node.node_id}", file=sys.stderr)
                        
                        print(file=sys.stderr)  # ç©ºè¡Œåˆ†éš”
                    
                    sources.append(source_info)
                
                # ğŸ” è¯Šæ–­æ¨¡å¼ï¼šè¾“å‡ºæ–‡æ¡£æ¥æºç»Ÿè®¡
                if diagnostic_mode:
                    print("ğŸ“Š æ–‡æ¡£æ¥æºç»Ÿè®¡:", file=sys.stderr)
                    print("-" * 40, file=sys.stderr)
                    file_count = {}
                    for source in sources:
                        if 'file_path' in source:
                            file_name = os.path.basename(source['file_path'])
                            file_count[file_name] = file_count.get(file_name, 0) + 1
                    
                    for file_name, count in sorted(file_count.items(), key=lambda x: x[1], reverse=True):
                        print(f"ğŸ“‚ {file_name}: {count} ä¸ªç‰‡æ®µ", file=sys.stderr)
                    
                    print("\nğŸ’¡ æ£€ç´¢åè§åˆ†æ:", file=sys.stderr)
                    print("-" * 40, file=sys.stderr)
                    total_sources = len(sources)
                    if total_sources > 0:
                        most_cited = max(file_count.items(), key=lambda x: x[1])
                        bias_ratio = most_cited[1] / total_sources
                        print(f"ğŸ” æœ€å¸¸å¼•ç”¨æ–‡æ¡£: {most_cited[0]} ({most_cited[1]}/{total_sources} = {bias_ratio:.1%})", file=sys.stderr)
                        if bias_ratio > 0.6:
                            print("âš ï¸  è­¦å‘Š: å­˜åœ¨æ˜æ˜¾çš„æ£€ç´¢åè§ï¼å•ä¸€æ–‡æ¡£å æ¯”è¿‡é«˜", file=sys.stderr)
                        elif bias_ratio > 0.4:
                            print("ğŸŸ¡ æ³¨æ„: å­˜åœ¨è½»å¾®çš„æ£€ç´¢åè§", file=sys.stderr)
                        else:
                            print("âœ… æ£€ç´¢ç»“æœç›¸å¯¹å‡è¡¡", file=sys.stderr)
                    
                    print("="*80, file=sys.stderr)
                    print("ğŸ”¬ è¯Šæ–­æŠ¥å‘Šç»“æŸ", file=sys.stderr)
                    print("="*80 + "\n", file=sys.stderr)
            
            result = {
                "answer": str(response)[:200] + "..." if len(str(response)) > 200 else str(response),
                "sources": sources,
                "query": question,
                "context": context,
                "sources_count": len(sources)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {str(e)}")
            return {
                "error": str(e),
                "answer": "",
                "sources": [],
                "query": question,
                "context": context,
                "sources_count": 0
            }

def create_rag_query(user_input: dict, image_analysis: list) -> str:
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥å’Œå›¾ç‰‡åˆ†æï¼Œæ„é€ ä¸“ä¸šçš„RAGæŸ¥è¯¢é—®é¢˜
    
    Args:
        user_input: ç”¨æˆ·è¾“å…¥ä¿¡æ¯
        image_analysis: å›¾ç‰‡åˆ†æç»“æœ
        
    Returns:
        æ„é€ çš„æŸ¥è¯¢é—®é¢˜
    """
    # ç®€åŒ–æŸ¥è¯¢ï¼Œé¿å…è¿‡é•¿
    bio = user_input.get('bio', user_input.get('bioOrChatHistory', ''))
    
    # æˆªæ–­è¿‡é•¿çš„å†…å®¹
    if len(bio) > 100:
        bio = bio[:100] + "..."
    
    base_query = f"""åˆ†æçº¦ä¼šå¯¹è±¡ï¼šæ˜µç§°{user_input.get('nickname', 'æœªçŸ¥')}ï¼ŒèŒä¸š{user_input.get('profession', 'æœªçŸ¥')}ï¼Œå¹´é¾„{user_input.get('age', 'æœªçŸ¥')}ã€‚ä¸ªäººç®€ä»‹ï¼š{bio}ã€‚è¯·è¯†åˆ«PUAè¡Œä¸ºæ¨¡å¼å’Œæƒ…æ„Ÿæ“æ§è¿¹è±¡ã€‚"""
    
    return base_query

def generate_final_report(rag_result: dict, user_input: dict, image_analysis: list) -> dict:
    """
    ç”Ÿæˆæœ€ç»ˆçš„ç»“æ„åŒ–åˆ†ææŠ¥å‘Š
    
    Args:
        rag_result: RAGæŸ¥è¯¢ç»“æœ
        user_input: ç”¨æˆ·è¾“å…¥ä¿¡æ¯
        image_analysis: å›¾ç‰‡åˆ†æç»“æœ
        
    Returns:
        ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Š
    """
    import datetime
    
    report = {
        "title": "AIæƒ…æ„Ÿå®‰å…¨åˆ†ææŠ¥å‘Š (OpenAIç‰ˆæœ¬)",
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "user_data": {
            "target_nickname": user_input.get('nickname', 'æœªçŸ¥'),
            "target_profession": user_input.get('profession', 'æœªçŸ¥'),
            "target_age": user_input.get('age', 'æœªçŸ¥'),
            "target_bio": user_input.get('bio', user_input.get('bioOrChatHistory', 'æœªæä¾›'))
        },
        "multimodal_analysis": image_analysis,
        "rag_analysis": {
            "query_summary": "åŸºäºä¸“ä¸šçŸ¥è¯†åº“çš„æ™ºèƒ½åˆ†æ",
            "knowledge_answer": rag_result.get('answer', ''),
            "sources_count": rag_result.get('sources_count', 0),
            "knowledge_references": rag_result.get('sources', [])
        },
        "risk_assessment": {
            "overall_risk": "éœ€è¦è¿›ä¸€æ­¥ä¸“ä¸šè¯„ä¼°",
            "identified_patterns": [],
            "safety_recommendations": []
        },
        "professional_insights": {
            "psychological_analysis": "åŸºäºçŸ¥è¯†åº“çš„ä¸“ä¸šåˆ†æ",
            "relationship_dynamics": "å·²æ•´åˆä¸“ä¸šç†è®º",
            "recommended_actions": []
        },
        "system_info": {
            "rag_status": "active" if not rag_result.get('error') else "error",
            "embedding_model": "OpenAI text-embedding-3-small",
            "knowledge_sources": f"ä¸“ä¸šèµ„æ–™åº“ ({rag_result.get('sources_count', 0)} ä¸ªç›¸å…³æ–‡æ¡£)",
            "ai_version": "OpenAI GPT-4o + RAGçŸ¥è¯†åº“"
        }
    }
    
    # å¦‚æœæœ‰RAGé”™è¯¯ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
    if rag_result.get('error'):
        report["rag_error"] = rag_result['error']
        report["system_info"]["rag_status"] = "error"
    
    return report

# å‘½ä»¤è¡Œè°ƒç”¨æ”¯æŒ
def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œè°ƒç”¨"""
    
    # ç¡®ä¿stdoutåªç”¨äºJSONè¾“å‡º
    try:
        if len(sys.argv) < 2:
            result = {
                "success": False,
                "error": "ç”¨æ³•: python rag_query_service.py '<JSONæ ¼å¼çš„æŸ¥è¯¢æ•°æ®>'"
            }
            print(json.dumps(result, ensure_ascii=False))
            sys.exit(1)
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        input_data = json.loads(sys.argv[1])
        
        # åˆå§‹åŒ–RAGæœåŠ¡
        rag_service = RAGQueryService()
        
        if not rag_service.is_initialized:
            result = {
                "success": False,
                "error": f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {rag_service.initialization_error}",
                "fallback_report": generate_final_report(
                    {"error": "åˆå§‹åŒ–å¤±è´¥", "answer": "", "sources": [], "sources_count": 0},
                    input_data.get('user_input', input_data.get('user_info', {})),
                    input_data.get('image_analysis', input_data.get('image_infos', []))
                )
            }
            print(json.dumps(result, ensure_ascii=False))
            sys.exit(0)
        
        # æå–ç”¨æˆ·è¾“å…¥å’Œå›¾ç‰‡åˆ†æ
        user_input = input_data.get('user_input', input_data.get('user_info', {}))
        image_analysis = input_data.get('image_analysis', input_data.get('image_infos', []))
        
        # æ„é€ RAGæŸ¥è¯¢
        rag_query = create_rag_query(user_input, image_analysis)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨è¯Šæ–­æ¨¡å¼
        diagnostic_mode = input_data.get('diagnostic_mode', False)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        rag_result = rag_service.query(rag_query, diagnostic_mode=diagnostic_mode)
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = generate_final_report(rag_result, user_input, image_analysis)
        
        # æ„å»ºæˆåŠŸå“åº”æ ¼å¼ï¼ŒåŒ¹é…server.jsæœŸæœ›çš„æ ¼å¼
        result = {
            "success": True,
            "data": final_report
        }
        
        # è¾“å‡ºJSONç»“æœåˆ°stdoutï¼ˆç¡®ä¿è¿™æ˜¯å”¯ä¸€çš„stdoutè¾“å‡ºï¼‰
        print(json.dumps(result, ensure_ascii=False))
        
    except json.JSONDecodeError:
        result = {
            "success": False,
            "error": "æ— æ•ˆçš„JSONè¾“å…¥æ ¼å¼"
        }
        print(json.dumps(result, ensure_ascii=False))
        sys.exit(1)
    except Exception as e:
        result = {
            "success": False,
            "error": f"å¤„ç†å¤±è´¥: {str(e)}"
        }
        print(json.dumps(result, ensure_ascii=False))
        sys.exit(1)

if __name__ == "__main__":
    main() 