#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆRAGæŸ¥è¯¢æœåŠ¡ - å¤šæ ·æ€§å¼ºåˆ¶æ£€ç´¢æœºåˆ¶
è§£å†³RAGç³»ç»Ÿæ£€ç´¢åè§é—®é¢˜ï¼Œç¡®ä¿æ¥æºå¤šæ ·æ€§
"""

import os
import sys
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime
from collections import Counter, defaultdict
import re
from contextlib import redirect_stdout

# å…¨å±€é‡å®šå‘stdoutåˆ°stderrï¼Œé˜²æ­¢æ±¡æŸ“JSONè¾“å‡º
class StdoutRedirector:
    def __init__(self):
        self.original_stdout = sys.stdout
        
    def __enter__(self):
        sys.stdout = sys.stderr
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout = self.original_stdout

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger(__name__)

# åœ¨å¯¼å…¥llama_indexä¹‹å‰è®¾ç½®æ—¥å¿—çº§åˆ«
with StdoutRedirector():
    # è®¾ç½®llama_indexæ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œå‡å°‘debugè¾“å‡º
    import logging as llama_logging
    llama_logging.getLogger("llama_index").setLevel(llama_logging.WARNING)

class EnhancedRAGService:
    """å¢å¼ºç‰ˆRAGæœåŠ¡ - å¤šæ ·æ€§å¼ºåˆ¶æ£€ç´¢"""
    
    def __init__(self, storage_path: str = "storage"):
        self.storage_path = Path(storage_path)
        self.index = None
        self.query_engine = None
        self.knowledge_sources = {
            'jordan_peterson': ['12-Rules-for-Life.pdf', 'jordan peterson2.pdf', 'Jordan_Peterson_Toxic_Masculinity_FINAL'],
            'sadia_khan': ['Sadia Khan', 'sadia khan'],
            'red_pill': ['çº¢è¯ä¸¸', 'ç´…è—¥ä¸¸', 'Week'],
            'mystery_method': ['è°œç”·æ–¹æ³•.pdf'],
            'abovelight': ['ABçš„ç•°æƒ³ä¸–ç•Œ']
        }
        self.initialize_rag_system()
    
    def initialize_rag_system(self):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºç‰ˆRAGç³»ç»Ÿ...")
            
            # éªŒè¯APIé…ç½®
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼")
            
            # é‡å®šå‘æ ‡å‡†è¾“å‡ºåˆ°stderrï¼Œé˜²æ­¢æ±¡æŸ“JSONè¾“å‡º
            import sys
            from contextlib import redirect_stdout
            
            # å¯¼å…¥å¿…è¦æ¨¡å—æ—¶é‡å®šå‘è¾“å‡º
            with redirect_stdout(sys.stderr):
                from llama_index.core import (
                    StorageContext,
                    load_index_from_storage,
                    Settings
                )
                from llama_index.embeddings.openai import OpenAIEmbedding
                
                # é…ç½®embeddingæ¨¡å‹
                Settings.embed_model = OpenAIEmbedding(
                    model="text-embedding-3-small",
                    api_key=api_key,
                    api_base="https://api.gptsapi.net/v1"
                )
                
                # åŠ è½½ç´¢å¼•
                storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_path))
                self.index = load_index_from_storage(storage_context)
            
            logger.info("âœ… å¢å¼ºç‰ˆRAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def classify_query_intent(self, query: str) -> dict:
        """åˆ†ææŸ¥è¯¢æ„å›¾ï¼Œè¯†åˆ«ç”¨æˆ·æƒ³è¦çš„çŸ¥è¯†æº"""
        intent_mapping = {
            'jordan_peterson': ['jordan peterson', 'jp', 'ä¸ªäººè´£ä»»', 'å¿ƒç†å­¦', '12æ¡è§„åˆ™', 'è´£ä»»', 'è‡ªä¿¡å»ºè®¾'],
            'sadia_khan': ['sadia khan', 'ç°ä»£å…³ç³»', 'å¥³æ€§å¿ƒç†', 'å…³ç³»å’¨è¯¢'],
            'red_pill': ['çº¢è¯ä¸¸', 'ä¸¤æ€§åŠ¨æ€', 'æ‹©å¶ç­–ç•¥', 'red pill', 'è“è¯ä¸¸'],
            'mystery_method': ['è°œç”·æ–¹æ³•', 'ç¤¾äº¤æŠ€å·§', 'pua', 'æ­è®ª'],
            'general': ['çº¦ä¼š', 'æ„Ÿæƒ…', 'å…³ç³»', 'å¿ƒç†', 'åˆ†æ']
        }
        
        query_lower = query.lower()
        intent_scores = defaultdict(float)
        
        for source, keywords in intent_mapping.items():
            for keyword in keywords:
                if keyword.lower() in query_lower:
                    intent_scores[source] += 1.0
                    # å¦‚æœæ˜¯æ˜ç¡®æŒ‡åçš„ä¸“å®¶ï¼Œç»™æ›´é«˜æƒé‡
                    if keyword.lower() in ['jordan peterson', 'sadia khan', 'è°œç”·æ–¹æ³•', 'çº¢è¯ä¸¸']:
                        intent_scores[source] += 2.0
        
        return dict(intent_scores)
    
    def diversified_retrieval(self, query: str, top_k: int = 5) -> list:
        """å¤šæ ·æ€§å¼ºåˆ¶å‡è¡¡æ£€ç´¢ - å¼ºç¡¬æ–¹æ¡ˆè§£å†³æ£€ç´¢åè§"""
        try:
            logger.info(f"ğŸ” å¼€å§‹å¤šæ ·æ€§å¼ºåˆ¶å‡è¡¡æ£€ç´¢: {query[:100]}...")
            
            # === ç¬¬ä¸€æ­¥ï¼šæ‰©å¤§åˆå§‹æ£€ç´¢èŒƒå›´ ===
            logger.info("ğŸ“ˆ ç¬¬ä¸€æ­¥ï¼šæ‰©å¤§æ£€ç´¢èŒƒå›´åˆ°20ä¸ªå€™é€‰ç‰‡æ®µ...")
            
            # é‡å®šå‘è¾“å‡ºé¿å…æ±¡æŸ“JSON
            import sys
            from contextlib import redirect_stdout
            
            with redirect_stdout(sys.stderr):
                retriever = self.index.as_retriever(similarity_top_k=20)  # è·å–å‰20ä¸ªå€™é€‰
                all_candidates = retriever.retrieve(query)
            
            logger.info(f"âœ… æ£€ç´¢åˆ° {len(all_candidates)} ä¸ªå€™é€‰ç‰‡æ®µ")
            
            # === ç¬¬äºŒæ­¥ï¼šå¤šæ ·æ€§ç­›é€‰åå¤„ç† ===
            logger.info("ğŸ¯ ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå¤šæ ·æ€§å¼ºåˆ¶ç­›é€‰...")
            
            # åˆ›å»ºç©ºçš„"æœ€ç»ˆçŸ¥è¯†åˆ—è¡¨"
            final_knowledge_list = []
            author_count = {}  # ç»Ÿè®¡æ¯ä¸ªä½œè€…çš„ç‰‡æ®µæ•°é‡
            
            # å¯¹å€™é€‰ç‰‡æ®µæŒ‰ç›¸å…³æ€§æ’åºï¼ˆå·²ç»æ˜¯æŒ‰ç›¸å…³æ€§æ’åºçš„ï¼‰
            logger.info("ğŸ“‹ å€™é€‰ç‰‡æ®µåˆ—è¡¨:")
            author_groups = {}  # æŒ‰ä½œè€…åˆ†ç»„
            
            for i, node in enumerate(all_candidates):
                file_path = getattr(node, 'metadata', {}).get('file_path', 'unknown')
                author = self.identify_source(file_path)
                
                if author not in author_groups:
                    author_groups[author] = []
                author_groups[author].append((i, node))
                
                logger.info(f"  {i+1}. [{author}] è¯„åˆ†: {node.score:.4f}, æ¥æº: {file_path}")
            
            logger.info(f"ğŸ” å‘ç° {len(author_groups)} ä¸ªä¸åŒä½œè€…çš„å†…å®¹")
            for author, nodes in author_groups.items():
                logger.info(f"   {author}: {len(nodes)} ä¸ªå€™é€‰ç‰‡æ®µ")
            
            # === å¼ºåŒ–å¤šæ ·æ€§ç­–ç•¥ ===
            # å¦‚æœæœ‰å¤šä¸ªä½œè€…ï¼Œä¼˜å…ˆç¡®ä¿å¤šæ ·æ€§
            if len(author_groups) > 1:
                logger.info("ğŸŒˆ æ‰§è¡Œå¼ºåŒ–å¤šæ ·æ€§ç­–ç•¥ï¼ˆå¤šä½œè€…æ¨¡å¼ï¼‰...")
                
                # ç¬¬ä¸€è½®ï¼šæ¯ä¸ªä½œè€…é€‰æ‹©æœ€å¥½çš„1ä¸ªç‰‡æ®µ
                for author, nodes in author_groups.items():
                    if len(final_knowledge_list) < top_k:
                        best_node = nodes[0][1]  # é€‰æ‹©è¯¥ä½œè€…æœ€ç›¸å…³çš„ç‰‡æ®µ
                        final_knowledge_list.append(best_node)
                        author_count[author] = 1
                        logger.info(f"   âœ… é€‰æ‹© {author} çš„æœ€ä½³ç‰‡æ®µ (è¯„åˆ†: {best_node.score:.4f})")
                
                # ç¬¬äºŒè½®ï¼šå¦‚æœè¿˜æœ‰ç©ºä½ï¼Œæ¯ä¸ªä½œè€…å†é€‰æ‹©1ä¸ªç‰‡æ®µ
                for author, nodes in author_groups.items():
                    if len(final_knowledge_list) < top_k and len(nodes) > 1:
                        if author_count.get(author, 0) < 2:  # ç¡®ä¿æ¯ä¸ªä½œè€…æœ€å¤š2ä¸ª
                            second_best_node = nodes[1][1]  # é€‰æ‹©è¯¥ä½œè€…ç¬¬äºŒç›¸å…³çš„ç‰‡æ®µ
                            final_knowledge_list.append(second_best_node)
                            author_count[author] = author_count.get(author, 0) + 1
                            logger.info(f"   âœ… é€‰æ‹© {author} çš„ç¬¬äºŒä¸ªç‰‡æ®µ (è¯„åˆ†: {second_best_node.score:.4f})")
                
                # ç¬¬ä¸‰è½®ï¼šå¦‚æœä»æœ‰ç©ºä½ï¼ŒæŒ‰ç›¸å…³æ€§ç»§ç»­å¡«å……ï¼ˆä»éµå®ˆæ¯ä½œè€…æœ€å¤š2ä¸ªé™åˆ¶ï¼‰
                if len(final_knowledge_list) < top_k:
                    remaining_candidates = []
                    for author, nodes in author_groups.items():
                        for i, node in nodes[2:]:  # ä»ç¬¬3ä¸ªç‰‡æ®µå¼€å§‹
                            remaining_candidates.append((i, node, author))
                    
                    # æŒ‰åŸå§‹ç›¸å…³æ€§æ’åº
                    remaining_candidates.sort(key=lambda x: x[0])
                    
                    for original_index, node, author in remaining_candidates:
                        if len(final_knowledge_list) >= top_k:
                            break
                        if author_count.get(author, 0) < 2:
                            final_knowledge_list.append(node)
                            author_count[author] = author_count.get(author, 0) + 1
                            logger.info(f"   âœ… è¡¥å……é€‰æ‹© {author} çš„ç‰‡æ®µ (è¯„åˆ†: {node.score:.4f})")
                
            else:
                # å•ä¸€ä½œè€…æ¨¡å¼ï¼šç›´æ¥æŒ‰ç›¸å…³æ€§é€‰æ‹©ï¼Œä½†ä»é™åˆ¶æœ€å¤š2ä¸ª
                logger.info("ğŸ“– æ‰§è¡Œå•ä¸€ä½œè€…æ¨¡å¼...")
                author = list(author_groups.keys())[0]
                nodes = author_groups[author]
                
                for i, (original_index, node) in enumerate(nodes):
                    if i >= 2 or len(final_knowledge_list) >= top_k:  # æœ€å¤š2ä¸ªç‰‡æ®µ
                        break
                    final_knowledge_list.append(node)
                    author_count[author] = author_count.get(author, 0) + 1
                    logger.info(f"   âœ… é€‰æ‹© {author} çš„ç‰‡æ®µ {i+1} (è¯„åˆ†: {node.score:.4f})")
            
            # === å¤šæ ·æ€§éªŒè¯å’Œç»Ÿè®¡ ===
            logger.info("ğŸ“Š å¤šæ ·æ€§å¼ºåˆ¶å‡è¡¡ç»“æœ:")
            logger.info(f"   æœ€ç»ˆç‰‡æ®µæ•°é‡: {len(final_knowledge_list)}")
            
            final_author_count = {}
            for node in final_knowledge_list:
                file_path = getattr(node, 'metadata', {}).get('file_path', 'unknown')
                author = self.identify_source(file_path)
                final_author_count[author] = final_author_count.get(author, 0) + 1
            
            for author, count in final_author_count.items():
                percentage = (count / len(final_knowledge_list) * 100) if final_knowledge_list else 0
                logger.info(f"   {author}: {count} ä¸ªç‰‡æ®µ ({percentage:.1f}%)")
            
            # éªŒè¯çº¦æŸæ¡ä»¶
            max_author_count = max(final_author_count.values()) if final_author_count else 0
            unique_authors = len(final_author_count)
            
            if max_author_count <= 2:
                logger.info("âœ… çº¦æŸéªŒè¯æˆåŠŸ: æ¯ä¸ªä½œè€…æœ€å¤š2ä¸ªç‰‡æ®µ")
            else:
                logger.warning(f"âš ï¸ çº¦æŸéªŒè¯å¤±è´¥: å‘ç°ä½œè€…è¶…å‡ºé™åˆ¶ ({max_author_count}ä¸ªç‰‡æ®µ)")
            
            if unique_authors >= 2:
                logger.info(f"âœ… å¤šæ ·æ€§ç›®æ ‡è¾¾æˆ: {unique_authors} ä¸ªä¸åŒä½œè€…")
            else:
                logger.info(f"âš ï¸ å¤šæ ·æ€§æœ‰é™: åªæœ‰ {unique_authors} ä¸ªä½œè€…ï¼ˆå¯èƒ½æ˜¯æŸ¥è¯¢è¿‡äºä¸“ä¸€ï¼‰")
            
            return final_knowledge_list
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ ·æ€§å¼ºåˆ¶å‡è¡¡æ£€ç´¢å¤±è´¥: {str(e)}")
            # é™çº§åˆ°åŸºç¡€æ£€ç´¢
            with redirect_stdout(sys.stderr):
                retriever = self.index.as_retriever(similarity_top_k=top_k)
                return retriever.retrieve(query)
    
    def identify_source(self, file_path: str) -> str:
        """è¯†åˆ«æ–‡æ¡£æ¥æº"""
        if not file_path:
            return 'unknown'
        
        file_name = Path(file_path).name.lower()
        
        # æ£€æŸ¥æ¯ä¸ªçŸ¥è¯†æºçš„æ ‡è¯†ç¬¦
        for source, identifiers in self.knowledge_sources.items():
            for identifier in identifiers:
                if identifier.lower() in file_name:
                    return source
        
        return 'other'
    
    def process_query(self, query_data: str) -> dict:
        """å¤„ç†æŸ¥è¯¢è¯·æ±‚"""
        try:
            logger.info("ğŸ¯ å¼€å§‹å¤„ç†å¢å¼ºç‰ˆRAGæŸ¥è¯¢...")
            
            # è§£æè¾“å…¥æ•°æ®
            data = json.loads(query_data)
            user_info = data.get('user_info', {})
            
            # æå–æŸ¥è¯¢å†…å®¹
            query = user_info.get('bioOrChatHistory', '') or user_info.get('bio', '')
            if not query:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æŸ¥è¯¢å†…å®¹")
            
            logger.info(f"ğŸ“ æŸ¥è¯¢å†…å®¹: {query[:100]}...")
            
            # æ‰§è¡Œå¤šæ ·æ€§å¼ºåˆ¶æ£€ç´¢
            nodes = self.diversified_retrieval(query, top_k=5)
            
            # æ„å»ºçŸ¥è¯†å›ç­”
            knowledge_answer = self.build_knowledge_answer(nodes, query)
            
            # æ„å»ºå¼•ç”¨ä¿¡æ¯
            knowledge_references = []
            for i, node in enumerate(nodes):
                ref = {
                    'score': float(node.score) if hasattr(node, 'score') else 0.0,
                    'file_path': node.metadata.get('file_path', 'unknown'),
                    'text_snippet': node.text[:200] + '...' if len(node.text) > 200 else node.text
                }
                knowledge_references.append(ref)
            
            # æ„å»ºå“åº”
            result = {
                'success': True,
                'data': {
                    'title': 'AIæƒ…æ„Ÿå®‰å…¨åˆ†ææŠ¥å‘Š (å¢å¼ºå¤šæ ·æ€§ç‰ˆæœ¬)',
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'user_info': user_info,
                    'rag_analysis': {
                        'status': 'active',
                        'knowledge_answer': knowledge_answer,
                        'knowledge_references': knowledge_references,
                        'sources_count': len(nodes),
                        'diversity_enhanced': True
                    }
                }
            }
            
            logger.info("âœ… å¢å¼ºç‰ˆRAGæŸ¥è¯¢å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'data': {}
            }
    
    def build_knowledge_answer(self, nodes: list, query: str) -> str:
        """æ„å»ºçŸ¥è¯†å›ç­”"""
        if not nodes:
            return "æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†å†…å®¹ã€‚"
        
        # æŒ‰æ¥æºç»„ç»‡å†…å®¹
        content_by_source = defaultdict(list)
        for node in nodes:
            source = self.identify_source(node.metadata.get('file_path', ''))
            content_by_source[source].append(node.text[:300])
        
        # æ„å»ºç»“æ„åŒ–å›ç­”
        answer_parts = []
        source_names = {
            'jordan_peterson': 'Jordan Petersonå¿ƒç†å­¦è§‚ç‚¹',
            'sadia_khan': 'Sadia Khanå…³ç³»åˆ†æ',
            'red_pill': 'çº¢è¯ä¸¸ç†è®ºè§‚ç‚¹', 
            'mystery_method': 'è°œç”·æ–¹æ³•æŠ€å·§',
            'abovelight': 'AboveLightè§‚ç‚¹',
            'other': 'å…¶ä»–ä¸“ä¸šè§‚ç‚¹'
        }
        
        for source, contents in content_by_source.items():
            if contents:
                source_name = source_names.get(source, source)
                combined_content = ' '.join(contents[:2])  # æ¯ä¸ªæ¥æºæœ€å¤š2æ®µå†…å®¹
                answer_parts.append(f"ã€{source_name}ã€‘\n{combined_content}")
        
        return '\n\n'.join(answer_parts)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print(json.dumps({
            'success': False,
            'error': 'Usage: python rag_query_service_enhanced.py <json_data>'
        }))
        sys.exit(1)
    
    try:
        # åˆ›å»ºå¢å¼ºç‰ˆRAGæœåŠ¡
        rag_service = EnhancedRAGService()
        
        # å¤„ç†æŸ¥è¯¢
        query_data = sys.argv[1]
        result = rag_service.process_query(query_data)
        
        # è¾“å‡ºç»“æœ
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        error_result = {
            'success': False,
            'error': str(e),
            'data': {}
        }
        print(json.dumps(error_result, ensure_ascii=False, indent=2))
        sys.exit(1)

if __name__ == "__main__":
    main() 