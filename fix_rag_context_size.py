#!/usr/bin/env python3
"""
RAGç³»ç»Ÿä¸Šä¸‹æ–‡å¤§å°é—®é¢˜ä¿®å¤è„šæœ¬
è§£å†³ "Calculated available context size -xxx was not non-negative" é”™è¯¯
"""

import os
import sys
from dotenv import load_dotenv
load_dotenv()

def fix_rag_system():
    """ä¿®å¤RAGç³»ç»Ÿçš„ä¸Šä¸‹æ–‡å¤§å°é—®é¢˜"""
    
    print("ğŸ”§ ä¿®å¤RAGç³»ç»Ÿä¸Šä¸‹æ–‡å¤§å°é—®é¢˜...")
    
    try:
        from llama_index.core import Settings
        from llama_index.core.llms import MockLLM
        from llama_index.embeddings.openai import OpenAIEmbedding
        
        # é…ç½®æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£
        print("âš™ï¸ é…ç½®LLMè®¾ç½®...")
        
        # ä½¿ç”¨MockLLMé¿å…LLMè°ƒç”¨ï¼ˆåªç”¨äºembeddingï¼‰
        mock_llm = MockLLM(max_tokens=4096)
        Settings.llm = mock_llm
        
        # é…ç½®embeddingæ¨¡å‹
        embed_model = OpenAIEmbedding(
            model="text-embedding-ada-002",
            api_key=os.getenv("OPENAI_API_KEY"),
            api_base=os.getenv("OPENAI_API_BASE") or "https://api.gptsapi.net/v1"
        )
        Settings.embed_model = embed_model
        
        # è®¾ç½®æ›´å¤§çš„chunk sizeå’Œcontext window
        Settings.chunk_size = 1024
        Settings.chunk_overlap = 200
        Settings.context_window = 8192  # å¢å¤§ä¸Šä¸‹æ–‡çª—å£
        Settings.num_output = 512
        
        print("âœ… LLMå’Œembeddingé…ç½®å®Œæˆ")
        
        # æµ‹è¯•ç®€å•æŸ¥è¯¢
        print("ğŸ§ª æµ‹è¯•RAGç³»ç»ŸæŸ¥è¯¢...")
        
        from llama_index.core import StorageContext, load_index_from_storage
        
        # åŠ è½½å­˜å‚¨çš„ç´¢å¼•
        storage_context = StorageContext.from_defaults(persist_dir="./storage")
        index = load_index_from_storage(storage_context)
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼Œé™åˆ¶æ£€ç´¢ç»“æœæ•°é‡
        query_engine = index.as_query_engine(
            similarity_top_k=2,  # å‡å°‘æ£€ç´¢ç»“æœæ•°é‡
            response_mode="compact",  # ä½¿ç”¨ç´§å‡‘æ¨¡å¼
            verbose=True
        )
        
        # æµ‹è¯•æŸ¥è¯¢
        test_query = "æµ‹è¯•æŸ¥è¯¢"
        print(f"ğŸ” æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢: {test_query}")
        
        response = query_engine.query(test_query)
        
        print("âœ… RAGç³»ç»Ÿæµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“‹ å“åº”: {str(response)[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿä¿®å¤å¤±è´¥: {str(e)}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        
        # æä¾›ä¿®å¤å»ºè®®
        if "context size" in str(e):
            print("\nğŸ’¡ å»ºè®®ä¿®å¤æ–¹æ¡ˆ:")
            print("1. å‡å°‘chunk_sizeåˆ°512")
            print("2. å‡å°‘similarity_top_kåˆ°1")
            print("3. ä½¿ç”¨æ›´ç®€å•çš„response_mode")
            
        return False

def test_query_with_params(query_text, user_info):
    """æµ‹è¯•å¸¦å‚æ•°çš„æŸ¥è¯¢"""
    try:
        print(f"\nğŸ§ª æµ‹è¯•å‚æ•°åŒ–æŸ¥è¯¢: {query_text}")
        
        from llama_index.core import Settings, StorageContext, load_index_from_storage
        from llama_index.core.llms import MockLLM
        from llama_index.embeddings.openai import OpenAIEmbedding
        
        # é‡æ–°é…ç½®æ›´ä¿å®ˆçš„è®¾ç½®
        mock_llm = MockLLM(max_tokens=1024)  # å‡å°max_tokens
        Settings.llm = mock_llm
        
        embed_model = OpenAIEmbedding(
            model="text-embedding-ada-002",
            api_key=os.getenv("OPENAI_API_KEY"),
            api_base=os.getenv("OPENAI_API_BASE") or "https://api.gptsapi.net/v1"
        )
        Settings.embed_model = embed_model
        
        # æ›´ä¿å®ˆçš„è®¾ç½®
        Settings.chunk_size = 512
        Settings.chunk_overlap = 50
        Settings.context_window = 2048
        Settings.num_output = 256
        
        storage_context = StorageContext.from_defaults(persist_dir="./storage")
        index = load_index_from_storage(storage_context)
        
        # æ›´ä¿å®ˆçš„æŸ¥è¯¢å¼•æ“
        query_engine = index.as_query_engine(
            similarity_top_k=1,  # åªæ£€ç´¢1ä¸ªæœ€ç›¸å…³çš„ç»“æœ
            response_mode="refine",  # ä½¿ç”¨refineæ¨¡å¼
            verbose=False
        )
        
        response = query_engine.query(query_text)
        
        result = {
            "query_summary": "åŸºäºä¸“ä¸šçŸ¥è¯†åº“çš„æ™ºèƒ½åˆ†æ",
            "knowledge_answer": str(response),
            "sources_count": 1,
            "knowledge_references": ["ä¸“ä¸šèµ„æ–™åº“æ£€ç´¢ç»“æœ"]
        }
        
        print("âœ… å‚æ•°åŒ–æŸ¥è¯¢æˆåŠŸ!")
        return result
        
    except Exception as e:
        print(f"âŒ å‚æ•°åŒ–æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return None

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨RAGç³»ç»Ÿä¿®å¤...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ é”™è¯¯: æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # ä¿®å¤RAGç³»ç»Ÿ
    success = fix_rag_system()
    
    if success:
        print("\nğŸ‰ RAGç³»ç»Ÿä¿®å¤å®Œæˆ!")
        
        # æµ‹è¯•å®é™…æŸ¥è¯¢
        test_user_info = {
            "nickname": "æµ‹è¯•ç”¨æˆ·",
            "profession": "å·¥ç¨‹å¸ˆ", 
            "age": "25",
            "bioOrChatHistory": "æµ‹è¯•ç®€ä»‹"
        }
        
        test_result = test_query_with_params("æµ‹è¯•æŸ¥è¯¢", test_user_info)
        if test_result:
            print("ğŸ“Š æµ‹è¯•ç»“æœ:", test_result)
    else:
        print("\nâŒ RAGç³»ç»Ÿä¿®å¤å¤±è´¥")
        sys.exit(1) 